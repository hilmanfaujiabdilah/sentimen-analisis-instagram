{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOMhi/HqxuRxA/sbkDLHRqC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Inverence Model"],"metadata":{"id":"lPVg30sfWg5W"}},{"cell_type":"markdown","source":["### Import Library"],"metadata":{"id":"zgJG21YtWzDc"}},{"cell_type":"code","source":["!pip install sastrawi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v20pw5wxXFBM","executionInfo":{"status":"ok","timestamp":1745423746392,"user_tz":-420,"elapsed":10612,"user":{"displayName":"Hilman Fauji Abdilah","userId":"17066367676907528451"}},"outputId":"39992bd2-ab76-4afe-9f20-58ce6ef6a92a"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting sastrawi\n","  Downloading Sastrawi-1.0.1-py2.py3-none-any.whl.metadata (909 bytes)\n","Downloading Sastrawi-1.0.1-py2.py3-none-any.whl (209 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.7/209.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sastrawi\n","Successfully installed sastrawi-1.0.1\n"]}]},{"cell_type":"code","source":["import re\n","import string\n","import joblib\n","from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","import nltk\n","nltk.download('punkt_tab')\n","nltk.download('stopwords')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NrYh_gUHW3GM","executionInfo":{"status":"ok","timestamp":1745424546386,"user_tz":-420,"elapsed":421,"user":{"displayName":"Hilman Fauji Abdilah","userId":"17066367676907528451"}},"outputId":"4ea5f113-b590-424c-c2cd-686f584cf72a"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["### Inverence Model"],"metadata":{"id":"EfnuIWsmWzXV"}},{"cell_type":"code","source":["# Load model\n","model = joblib.load('random_forest_model.pkl')\n","tfidf = joblib.load('tfidf_vectorizer.pkl')\n","slangwords = joblib.load('slangwords_dict.pkl')"],"metadata":{"id":"h4QgxAlHXa0M","executionInfo":{"status":"ok","timestamp":1745424150597,"user_tz":-420,"elapsed":476,"user":{"displayName":"Hilman Fauji Abdilah","userId":"17066367676907528451"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"knM8nAZSWd3N","executionInfo":{"status":"ok","timestamp":1745424558982,"user_tz":-420,"elapsed":2393,"user":{"displayName":"Hilman Fauji Abdilah","userId":"17066367676907528451"}},"outputId":"d24a0493-e2ec-4ca9-9f0c-9c3c03830bb1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Masukan teks : instagram aplikasi yang sangat bagus\n","Hasil prediksi sentimen: positive\n"]}],"source":["text = input(\"Masukan teks : \")\n","\n","# Lowercase\n","text = text.lower()\n","\n","# Cleaning text\n","text = re.sub(r'\\d+', '', text)\n","text = re.sub(r'@[A-Za-z0-9_]+', '', text)\n","text = re.sub(r'#[A-Za-z0-9_]+', '', text)\n","text = re.sub(r'RT[\\s]+', '', text)\n","text = re.sub(r'http\\S+', '', text)\n","text = re.sub(r'[^\\w\\s]', '', text)\n","text = text.replace('\\n', ' ')\n","text = text.translate(str.maketrans('', '', string.punctuation))\n","text = text.strip()\n","\n","# Slangword normalization\n","words = text.split()\n","fixed_words = []\n","for word in words:\n","    if word in slangwords:\n","        fixed_words.append(slangwords[word])\n","    else:\n","        fixed_words.append(word)\n","text = ' '.join(fixed_words)\n","\n","# Tokenization\n","tokens = word_tokenize(text)\n","\n","# Stopword removal\n","stop_words = set(stopwords.words('indonesian')).union(stopwords.words('english'))\n","tambahan = ['iya', 'yaa', 'gak', 'nya', 'na', 'sih', 'ku', 'di', 'ga', 'ya', 'gaa', 'loh', 'kah', 'woi', 'woii', 'woy']\n","stop_words.update(tambahan)\n","tokens = [word for word in tokens if word not in stop_words]\n","\n","# Stemming\n","factory = StemmerFactory()\n","stemmer = factory.create_stemmer()\n","stemmed_text = stemmer.stem(' '.join(tokens))\n","\n","# TF-IDF dan prediksi\n","vectorized = tfidf.transform([stemmed_text])\n","prediction = model.predict(vectorized)\n","\n","# Output hasil prediksi\n","print(f\"Hasil prediksi sentimen: {prediction[0]}\")"]}]}